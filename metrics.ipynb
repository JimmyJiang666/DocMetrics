{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimmy/.pyenv/versions/3.8.10/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from bert_score import score as bert_score\n",
    "from parent import parent as parent_function\n",
    "\n",
    "class TextInputHandler:\n",
    "    def __init__(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Initialize with the path to the file.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "\n",
    "    def read_file(self) -> str:\n",
    "        \"\"\"\n",
    "        Read the entire content of the file.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.file_path):\n",
    "            raise FileNotFoundError(f\"File {self.file_path} not found.\")\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "\n",
    "    def read_lines(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Read the file line by line into a list.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.file_path):\n",
    "            raise FileNotFoundError(f\"File {self.file_path} not found.\")\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.readlines()\n",
    "\n",
    "class EvaluationMetric:\n",
    "    def evaluate(self, reference: str, hypothesis: str, table_data: dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate the hypothesis against the reference.\n",
    "        Should be overridden by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"This method should be overridden by subclasses.\")\n",
    "\n",
    "class ROUGEEvaluation(EvaluationMetric):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the ROUGE evaluator.\n",
    "        \"\"\"\n",
    "        self.rouge = Rouge()\n",
    "\n",
    "    def evaluate(self, reference: str, hypothesis: str, table_data: dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate using ROUGE metrics.\n",
    "        \"\"\"\n",
    "        scores = self.rouge.get_scores(hypothesis, reference, avg=True)\n",
    "        return scores\n",
    "\n",
    "class BLUEEvaluation(EvaluationMetric):\n",
    "    def evaluate(self, reference: str, hypothesis: str, table_data: dict = None) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate using BLEU score.\n",
    "        \"\"\"\n",
    "        reference_tokens = [reference.split()]\n",
    "        hypothesis_tokens = hypothesis.split()\n",
    "        score = sentence_bleu(reference_tokens, hypothesis_tokens)\n",
    "        return score\n",
    "\n",
    "class BertScoreEvaluation(EvaluationMetric):\n",
    "    def evaluate(self, reference: str, hypothesis: str, table_data: dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate using BERTScore metrics.\n",
    "        \"\"\"\n",
    "        P, R, F1 = bert_score([hypothesis], [reference], lang='en')\n",
    "        return {\"precision\": P.mean().item(), \"recall\": R.mean().item(), \"f1\": F1.mean().item()}\n",
    "\n",
    "class PARENTEvaluation(EvaluationMetric):\n",
    "    def __init__(self, table_file: str):\n",
    "        \"\"\"\n",
    "        Initialize the PARENT evaluator with the table file.\n",
    "        \"\"\"\n",
    "        self.table_file = table_file\n",
    "\n",
    "    def read_table_file(self) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Read the table file into a list of dictionaries.\n",
    "        \"\"\"\n",
    "        with open(self.table_file, mode=\"r\", encoding='utf8') as f:\n",
    "            tables = [json.loads(line) for line in f if line.strip()]\n",
    "        return tables\n",
    "\n",
    "    def evaluate(self, references: List[str], hypotheses: List[str], tables: List[dict]) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate using PARENT metrics.\n",
    "        \"\"\"\n",
    "        assert len(references) == len(hypotheses) == len(tables), \"Mismatch in the number of references, hypotheses, and tables\"\n",
    "        \n",
    "        references_split = [ref.split() for ref in references]\n",
    "        hypotheses_split = [hyp.split() for hyp in hypotheses]\n",
    "\n",
    "        precision, recall, f_score = parent_function(\n",
    "            hypotheses_split,\n",
    "            references_split,\n",
    "            tables,\n",
    "            avg_results=True,\n",
    "            n_jobs=32,\n",
    "            use_tqdm=False\n",
    "        )\n",
    "        return {\"precision\": precision, \"recall\": recall, \"f1\": f_score}\n",
    "\n",
    "class EvaluationPipeline:\n",
    "    def __init__(self, reference_file: str, hypothesis_file: str, metric: EvaluationMetric, table_file: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the evaluation pipeline with reference and hypothesis files, and the metric.\n",
    "        \"\"\"\n",
    "        self.reference_handler = TextInputHandler(reference_file)\n",
    "        self.hypothesis_handler = TextInputHandler(hypothesis_file)\n",
    "        self.metric = metric\n",
    "        self.table_file = table_file\n",
    "\n",
    "    def run(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Run the evaluation pipeline and return the scores.\n",
    "        \"\"\"\n",
    "        references = self.reference_handler.read_lines()\n",
    "        hypotheses = self.hypothesis_handler.read_lines()\n",
    "        if isinstance(self.metric, PARENTEvaluation):\n",
    "            table_data = self.metric.read_table_file()\n",
    "            return self.metric.evaluate(references, hypotheses, table_data)\n",
    "        return self.metric.evaluate(\"\\n\".join(references), \"\\n\".join(hypotheses))\n",
    "\n",
    "def select_metric(metric_name: str, table_file: str = None) -> EvaluationMetric:\n",
    "    \"\"\"\n",
    "    Select the evaluation metric based on the given name.\n",
    "    \"\"\"\n",
    "    if metric_name.lower() == 'rouge':\n",
    "        return ROUGEEvaluation()\n",
    "    elif metric_name.lower() == 'bleu':\n",
    "        return BLUEEvaluation()\n",
    "    elif metric_name.lower() == 'bertscore':\n",
    "        return BertScoreEvaluation()\n",
    "    elif metric_name.lower() == 'parent':\n",
    "        if table_file is None:\n",
    "            raise ValueError(\"Table file must be provided for PARENT evaluation.\")\n",
    "        return PARENTEvaluation(table_file)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric: {metric_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.7974652832334177, 'recall': 0.4502892917429524, 'f1': 0.5528647763011489}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## USE CASE FOR PARENT\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify file paths\n",
    "    reference_file = 'data/wb_test_output.txt'\n",
    "    hypothesis_file = 'data/wb_predictions.txt'\n",
    "    table_file = 'data/wb_test_tables.jl'\n",
    "    \n",
    "    # Choose the evaluation metric by input string\n",
    "    metric_name = 'parent'  # or 'rouge', 'bleu', 'bertscore'\n",
    "    metric = select_metric(metric_name, table_file=table_file if metric_name == 'parent' else None)\n",
    "    \n",
    "    # Run the evaluation pipeline\n",
    "    pipeline = EvaluationPipeline(reference_file, hypothesis_file, metric, table_file=table_file)\n",
    "    scores = pipeline.run()\n",
    "    print(scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.8495630025863647, 'recall': 0.8725723624229431, 'f1': 0.8609139919281006}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## USE CASE FOR OTHER METRICS\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify file paths\n",
    "    reference_file = 'data/B_ref.txt'\n",
    "    hypothesis_file = 'data/B_out.txt'\n",
    "    \n",
    "    # Choose the evaluation metric by input string\n",
    "    # metric_name = 'rouge' \n",
    "    # metric_name = 'bleu'\n",
    "    metric_name = 'bertscore'\n",
    "    metric = select_metric(metric_name)\n",
    "    \n",
    "    # Run the evaluation pipeline\n",
    "    pipeline = EvaluationPipeline(reference_file, hypothesis_file, metric)\n",
    "    scores = pipeline.run()\n",
    "    print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('3.8.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8dd565163b7d760a36f44dae9da32400f7fc3d56b875bbbdb07aff43d8d4529"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
